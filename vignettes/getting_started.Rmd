---
title: "Getting Started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{eatPrep}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
library(knitr)
```

## Installation

`eatPrep` is on GitHub. You can install R packages from GitHub via the `remotes` package. With the following code you can download both packages `remotes` and `eatPrep`. 

```{r installing and loading, warning=FALSE}
#install.packages("remotes")
#remotes::install_github("https://github.com/sachseka/eatPrep")
library(eatPrep)
```

## Introduction

The goal of `eatPrep`, short for `Educational Assesment Tools: Preparation`, is to automate the preparation of item response test data; especially but not exclusively for typical use cases at the *Institute for Educational Quality Improvement* (IQB).
Therefore it works together with IQB-specific applications such as the `ItemDB` or `ZKDaemon`.

#### Paper Competence Data Preparation

We usually give students paper tests or questionnaires that they fill out by hand, while being supervised. The results are later collected centrally. These tests are then scanned. For multiple choice questions or tick questions an optic scanning system is used that recognizes marked fields automatically and transmits them to a digital data bank. Trained coders evaluate the (partially) open questions. This data is usually in a SPSS (.sav) format.

We use `eatPrep` for the standardization and automation of the handling of such data. 

#### Features

The main purpose of `eatPrep` is to read, score, aggregate and merge data from a regular `SPSS` (`.sav`) data file (via the function `readSpss()`) and metadata from databases (`readDaemonXlsx()`) and Excel spreadsheets (`readMerkmalXlsx()`). Additional functions include checking the data (`checkData()`), meta data (`checkInputList()`), and the design (`checkDesign()`), recoding of (missing) values (`recodeData()`, `mnrCoding()`, `aggregateData()`, `scoreData()`,`collapseMissings()`), working with rater data (`meanAgree()`, `meanKappa()`), evaluating item categories (`catPbc()`, `evalPbc()`) and exporting prepared or raw item data sets to different formats (`writeSpss()`, `prep2GADS()`). Most of this can be done with a single function call to `automateDataPreparation()`.

||
|--|
|**Automation** of Data Preparation|
|Various **tests**, plausibility checks and diagnostics|
|Handling of many different **missing types**, if desired retention until the end (incl. missing-not-reached calculation)|
|Additional **tools** (category separation, rater functions, semi-manual data cleansing, export functions...)|


## Data Structures of Paper Competency Data at the IQB

In a prototypical scenario, we are constructing a test at the IQB, e.g. for educational purposes. For our pilot study, we have created several overlapping booklets and a variety of items with different item formats (constructed response, multiple choice).  
Thereby, the data consists of several layers:

|||
|-|-|
|**booklets** |containing blocks|
|**blocks/cluster**| containing tasks|
|**tasks**| contain units (=items) (usually for a common stimulus/testlet)
| **units**| containing subunits/subitems|
| **subunits**| having values |
| **values**| including missings and recode values|
|**valueRecodes**| also "Scores": A mapping of the original values to a smaller number of categories (usually at least True/False/Missing, i.e. 1/0/NA) that are better suited for IRT scaling|

**The goal is to**:

1. Recode all items and subitems
2. Aggregate subitems
3. Recode aggregated subitems

![](structure.jpg){width="400"}

```{=html}
<table>
  <caption></caption>
  <thead>
    <tr>
      <th>Subitems, Items and Tasks</th>
      <th>Subitems, Items, Tasks, Values and Scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
    <td><img src="cyclehire4.jpg" alt="Subitems, Items and Tasks" width="350" /></td>
      <td style=width:100px; height:50px;><img src="cyclehire1.jpg" alt="Subitems, Items, Tasks, Values and Scores" width="350" /></td>
    </tr>
  </tbody>
</table>
```



#### Data Structure

#### Missing Values

Now we would like to prepare the data collected in this way for scaling. In doing so, we would like to know if the booklets are speeded (number of mnr). Additionally, we intend to use different missing treatments for the actual scaling, so we preserve missing values during data preparation and recode them right before scaling the data. `eatPrep` can help us with all of that. 

Here is an overview for different value types the IQB uses: 

```{r missing types, echo=FALSE}
mir98 <- c(-98, "mir", "missing invalid response",
           "(1) Item was edited, and (2a) empty answer or (2b) invalid (joke) answer.")
mbo99 <- c(-99, "mbo", "missing by omission",
           "Item wasn't edited but seen, or wasn't seen, but there are seen or edited subsequent Items.")
mnr96 <- c(-96, "mnr", "missing not reached",
           "(1) Item wasn't seen, and (2) all subsequent Items weren't seen, either.")
mci97 <- c(-97, "mci", "missing coding impossible",
           "(1) Item should/could have been edited, and (2) answer can't be analysed due to technical problems.")
mbd94 <- c(-94, "mbd", "missing by design",
           "no answer, because Item wasn't shown to the testperson by design.")
Mtypes <- rbind.data.frame(mir98, mbo99, mnr96, mci97, mbd94)
names(Mtypes) <- c("Code", "Label", "Abbr", "Explanation")

kable(Mtypes, caption = "**Missing Types**")
```

#### Representation of the Data Structure in eatPrep



--------------------------------------------------------------------------------
                      old vignette
--------------------------------------------------------------------------------


<!-- we also want to include additional information about the items (merkmalstabelle) -> PASST HIER EIGENTLICH GAR NICHT IN DIE DATENSTRUKTUR! -->
<!-- And here is some additional information about the **items**: -->

```{r item types, echo=FALSE}
code <- c()
label <- c()
kurz <- c()
recode_value <- c()
#kable(Itypes <- data.frame(Code = code, Label = label, Abbr = kurz), caption = "**items types**")
```


## Input

Information/metadata about items is stored in many different ways, e.g., databases, Excel sheets, sometimes even in item names. `eatPrep` requires this metadata to have a specific form, namely a series of relational tables. For `eatPrep` it is most convenient to store these tables in a list. When downloading the `eatPrep` package, you can find files with some example tables in the `data` folder. 

Sheets have specific names, also the columns in those sheets have specific names.

`inputMinimal` contains the bare minimum for all functions in `eatPrep` to work, `inputList` contains additional metadata about the items (such as information about the task's content). 

<!--Example: inputList ? -->

```{r tempsetup}
# Example: inputMinimal
inputMinimal <- list(units = inputList$units[ -nrow(inputList$units), c("unit", "unitAggregateRule")],
                     subunits = inputList$subunits[, c("unit", "subunit","subunitRecoded")],
                     values = inputList$values[ , c("subunit", "value", "valueRecode", "valueType")],
                     unitRecodings = inputList$unitRecodings[ , c("unit", "value", "valueRecode", "valueType")],
                     blocks = inputList$blocks,
                     booklets = inputList$booklets,
                     rotation = inputList$rotation)
```

### Input Tables
Let's look at the tables from bottom to top, starting with items and their values. In the values sheet, we define values and recode values for each subitem.

Here we see 3 items (called units) from `inputList` bzw. `inputMinimal`.
One mc (item 1), one short response (item 5), one item with 3 subitems short response (item 12)

```{r inputList}
items <- c("I01", "I05", "I12")
subitems <- c("I01", "I05", "I12a", "I12b", "I12c")

kable(inputMinimal$units[which(inputMinimal$units$unit %in% items), ])

```

The **values sheet** contains all possible values for each subitem and scoring information for each value (e.g., whether it is true, false, or missing). There are several types of missing values.

```{r values sheet}
kable(inputMinimal$values[which(inputMinimal$values$subunit %in% subitems), ])
```

<!-- talk more about values in the data for different item types (mc, cr) and about missing values in the data. missing values do not need to be the same for each variable in the data.-->

The **subunits sheet** contains information about the relationship between subitems and items. This information is used to aggregate subitems to items. If recoded subitems should be named differently than the unrecoded variables in the original data, this can be specified via `subunitRecoded`. The only type of aggregation currently supported by `eatPrep` is to count the number of correct subitems per item.

```{r subunits sheet}
kable(inputMinimal$subunits[which(inputMinimal$subunits$subunit %in% subitems), ])
```

**unitrecodings** contains the same information as the values sheet, but for aggregated items. Here, the value on an item is the aggregated number of correct subitems and the recode value gives a threshold of how many subitems need to be solved to obtain a credit for the item. In the example, I12 has 3 subitems, and one receives a credit if all subitems are solved correctly.

```{r unitRecodings}
kable(inputMinimal$unitRecodings)
```

The **minimal use case** is that each subitem corresponds to an item. In this case, the value sheet contains all information needed for recoding the items (apart from possibly renaming the recoded items).


## Input Data

We have several overlapping booklets with several blocks in each booklet. Moreover, there is a unique identifier for each person and some additional information about each student like their gender or their socioeconomic status. There is one data set per booklet. In order to prepare the data, we need to construct one large data set.

In order to do that we first need to read the data into R, check the data for invalid or or incorrect codes and then merge the data into one data set. In the following the different functions to do that are described. `inputDat` gives us a first idea on how the data is supposed to look like. 

```{r inputDat}
# looking at the data
str(inputDat)
```

